{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "SOME_FREE_SONGTEXT = \"So one night I said to Kathy We gotta get away somehow Go somewhere south and somewhere warm But for God's sake let's go now. And Kathy she sort of looks at me And asks where I wanna go So I look back and I hear me say I don't care but we gotta go chorus and key change And all the other people Who slepwalk thru their days Just sort of faded out of sight When we two drove away And ev'ry day we travelled We were lookin' to get wise And we learned what was the truth And we learned what were the lies And in LA we bought a bus Sort of old and not too smart So for six hundred and fifty bucks We got out and made a start We hit the road down to the South And drove into Mexico That old bus was some old wreck But it just kept us on the road. chorus etc We drove up to Alabam And a farmer gave us some jobs We worked them crops all night and day And at night we slept like dogs We got paid and Kathy said to me It's time to make a move again And when I looked into her eyes I saw more than a friend. chorus etc And now we've stopped our travels And we sold the bus in Texas And we made our home in Austin And for sure it ain't no palace And Kathy and me we settled down And now our first kid's on the way Kathy and me and that old bus We did real good to get away.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_songs(filename, nrows=None):\n",
    "        return pd.read_csv(filename, sep='\\t', nrows=nrows)\n",
    "\n",
    "\n",
    "def load_artists(filename, nrows=None):\n",
    "        return pd.read_csv(filename, sep=',', nrows=nrows)\n",
    "\n",
    "def load_albums(filename, nrows=None):\n",
    "        return pd.read_csv(filename, sep='\\t', nrows=nrows)\n",
    "\n",
    "def random_key_from_dict(dictionary, seed=123):\n",
    "        keys_list = list(dictionary.keys())\n",
    "        np.random.seed(seed)\n",
    "        random_index = np.random.choice(len(keys_list))\n",
    "        return keys_list[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quick overview over some columns\n",
    "df = load_songs('wasabi_songs.csv', nrows=10000)\n",
    "for property in ['artist', 'title', 'publicationDateAlbum', 'album_genre',\n",
    "                 'valence', 'arousal', 'valence_predicted', 'arousal_predicted', 'bpm']:\n",
    "    print('Histogram for:', property)\n",
    "    print(pd.value_counts(df[property]), '\\n\\n')\n",
    "    \n",
    "\n",
    "# remove rows that either have NaN in relevant feature or bpm cell(s)\n",
    "clean_df = df[['artist', 'title', 'publicationDateAlbum', 'album_genre','bpm']].dropna()\n",
    "clean_df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quick overview over some columns\n",
    "df = load_songs('wasabi_songs.csv', nrows=100000)\n",
    "for property in ['artist', 'title', 'publicationDateAlbum', 'album_genre',\n",
    "                 'valence', 'arousal', 'valence_predicted', 'arousal_predicted', 'bpm']:\n",
    "    print('Histogram for:', property)\n",
    "    print(pd.value_counts(df[property]), '\\n\\n')\n",
    "    \n",
    "\n",
    "# remove rows that either have NaN in relevant feature or bpm cell(s)\n",
    "clean_df = df[['artist', 'title', 'publicationDateAlbum', 'album_genre','bpm']].dropna()\n",
    "clean_df.info()\n",
    "\n",
    "column_data_type = []\n",
    "for col in clean_df.columns:\n",
    "    data_type = clean_df[col].dtype\n",
    "    if clean_df[col].dtype in ['int64','float64']:\n",
    "        column_data_type.append('numeric')\n",
    "    else:\n",
    "        column_data_type.append('categorical')\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.countplot(x=column_data_type)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Flatten\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Splitting the data into features (X) and the target (y)\n",
    "X = clean_df.drop(columns=['bpm'])  # Features\n",
    "y = clean_df['bpm']  # Target\n",
    "\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the text feature columns\n",
    "text_feature_columns = ['artist', 'title', 'album_genre']\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "tokenizer = Tokenizer()\n",
    "max_sequence_length = 50  # Define a suitable maximum sequence length\n",
    "vocab_size = 10000  # Define the vocabulary size\n",
    "embedding_dim = 100  # Define the embedding dimension\n",
    "\n",
    "for column in text_feature_columns:\n",
    "    X_train[column] = X_train[column].astype(str)\n",
    "    X_test[column] = X_test[column].astype(str)\n",
    "    tokenizer.fit_on_texts(X_train[column])\n",
    "    X_train[column] = pad_sequences(tokenizer.texts_to_sequences(X_train[column]), maxlen=max_sequence_length)\n",
    "    X_test[column] = pad_sequences(tokenizer.texts_to_sequences(X_test[column]), maxlen=max_sequence_length)\n",
    "\n",
    "# Create and train word embeddings (Word2Vec, GloVe, etc.) for your text features\n",
    "# You would need to download or train these embeddings using an NLP library\n",
    "\n",
    "# Define a Keras model that incorporates embedding layers for text features\n",
    "model = Sequential()\n",
    "\n",
    "for column in text_feature_columns:\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "    model.add(Flatten())  # Flatten the embedding output\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))  # Linear activation for regression\n",
    "\n",
    "# Compile the model with an appropriate loss function for regression\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit([X_train[column] for column in text_feature_columns], y_train, epochs=100, batch_size=2, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "mse = model.evaluate([X_test[column] for column in text_feature_columns], y_test)\n",
    "print(f'Mean Squared Error on Test Data: {mse}')\n",
    "\n",
    "# You can use this model to make predictions on new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Step: Make new df with all relevant features we want to incorporate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
