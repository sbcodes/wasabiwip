{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "SOME_FREE_SONGTEXT = \"So one night I said to Kathy We gotta get away somehow Go somewhere south and somewhere warm But for God's sake let's go now. And Kathy she sort of looks at me And asks where I wanna go So I look back and I hear me say I don't care but we gotta go chorus and key change And all the other people Who slepwalk thru their days Just sort of faded out of sight When we two drove away And ev'ry day we travelled We were lookin' to get wise And we learned what was the truth And we learned what were the lies And in LA we bought a bus Sort of old and not too smart So for six hundred and fifty bucks We got out and made a start We hit the road down to the South And drove into Mexico That old bus was some old wreck But it just kept us on the road. chorus etc We drove up to Alabam And a farmer gave us some jobs We worked them crops all night and day And at night we slept like dogs We got paid and Kathy said to me It's time to make a move again And when I looked into her eyes I saw more than a friend. chorus etc And now we've stopped our travels And we sold the bus in Texas And we made our home in Austin And for sure it ain't no palace And Kathy and me we settled down And now our first kid's on the way Kathy and me and that old bus We did real good to get away.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_songs(filename, nrows=None):\n",
    "        return pd.read_csv(filename, sep='\\t', nrows=nrows)\n",
    "\n",
    "\n",
    "def load_artists(filename, nrows=None):\n",
    "        return pd.read_csv(filename, sep=',', nrows=nrows)\n",
    "\n",
    "def load_albums(filename, nrows=None):\n",
    "        return pd.read_csv(filename, sep='\\t', nrows=nrows)\n",
    "\n",
    "def random_key_from_dict(dictionary, seed=123):\n",
    "        keys_list = list(dictionary.keys())\n",
    "        np.random.seed(seed)\n",
    "        random_index = np.random.choice(len(keys_list))\n",
    "        return keys_list[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram for: artist\n",
      "artist\n",
      "A Cor Do Som            107\n",
      "A                       102\n",
      "A Global Threat         100\n",
      "A Halo Called Fred       79\n",
      "A Day To Remember        74\n",
      "                       ... \n",
      "A Christmas Carol         1\n",
      "A Contrail To Follow      1\n",
      "A Farewell Rescue         1\n",
      "A Girl Called Jane        1\n",
      "A Chilling Silence        1\n",
      "Name: count, Length: 129, dtype: int64 \n",
      "\n",
      "\n",
      "Histogram for: title\n",
      "title\n",
      "Beleza Pura                           6\n",
      "Menino Deus                           6\n",
      "Zanzibar (As Cores)                   6\n",
      "Abri A Porta                          5\n",
      "Alto Astral                           5\n",
      "                                     ..\n",
      "Take Care                             1\n",
      "A Song The World Can Sing Out Loud    1\n",
      "White Lines And Lipstick              1\n",
      "Shoot From The Hip                    1\n",
      "First Light Of Dawn                   1\n",
      "Name: count, Length: 1881, dtype: int64 \n",
      "\n",
      "\n",
      "Histogram for: publicationDateAlbum\n",
      "publicationDateAlbum\n",
      "2008    158\n",
      "2005    155\n",
      "2004    131\n",
      "2007    125\n",
      "2006    123\n",
      "2009    104\n",
      "2001    104\n",
      "2010     91\n",
      "2011     69\n",
      "2002     56\n",
      "1999     55\n",
      "2012     55\n",
      "2000     49\n",
      "1997     33\n",
      "1994     32\n",
      "2013     27\n",
      "1995     24\n",
      "2014     22\n",
      "2003     20\n",
      "1998     19\n",
      "1983     13\n",
      "2016     12\n",
      "2015     11\n",
      "1982     10\n",
      "1984     10\n",
      "1996      9\n",
      "1985      9\n",
      "200?      6\n",
      "1981      4\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "\n",
      "Histogram for: album_genre\n",
      "album_genre\n",
      "Alternative Rock      104\n",
      "MPB                    86\n",
      "Post-Hardcore          80\n",
      "Pop                    78\n",
      "Metalcore              44\n",
      "New Wave               41\n",
      "Progressive Metal      37\n",
      "Thrash Metal           36\n",
      "Pop Rock               35\n",
      "Heavy Metal            33\n",
      "Rock                   31\n",
      "Gothic Metal           31\n",
      "Indie Rock             28\n",
      "Deathcore              27\n",
      "Emo                    24\n",
      "Dark Wave              23\n",
      "Classical              21\n",
      "Death Metal            20\n",
      "Synthpop               19\n",
      "Pop Punk               17\n",
      "Sludge Metal           17\n",
      "Post-Punk              16\n",
      "Europop                14\n",
      "Hardcore Punk          12\n",
      "Hip Hop                12\n",
      "Experimental Rock      11\n",
      "Black Metal            10\n",
      "Doom Metal              9\n",
      "Neofolk                 7\n",
      "Melodic Metalcore       6\n",
      "unk                     4\n",
      "Brutal Death Metal      4\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "\n",
      "Histogram for: valence\n",
      "valence\n",
      "-1.744233    3\n",
      " 1.541257    2\n",
      " 0.373325    2\n",
      "-0.780962    1\n",
      "-1.746962    1\n",
      "-0.517359    1\n",
      "-1.040199    1\n",
      "-0.456234    1\n",
      " 0.864511    1\n",
      " 1.306579    1\n",
      "-0.538098    1\n",
      "-1.086589    1\n",
      " 1.178325    1\n",
      " 0.135009    1\n",
      "-0.611094    1\n",
      "-0.713652    1\n",
      " 0.991401    1\n",
      "-0.685454    1\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "\n",
      "Histogram for: arousal\n",
      "arousal\n",
      " 1.335419    3\n",
      " 1.704165    2\n",
      "-0.923151    2\n",
      "-0.789480    1\n",
      " 1.127999    1\n",
      "-0.012348    1\n",
      "-0.065816    1\n",
      " 1.630415    1\n",
      "-0.517530    1\n",
      " 1.275497    1\n",
      "-0.930065    1\n",
      "-0.077340    1\n",
      " 1.478308    1\n",
      "-0.047379    1\n",
      " 0.092053    1\n",
      " 0.044808    1\n",
      " 0.554138    1\n",
      "-0.219460    1\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "\n",
      "Histogram for: valence_predicted\n",
      "valence_predicted\n",
      "-0.347331    344\n",
      "-0.118055     75\n",
      " 0.036752      6\n",
      "-0.012591      6\n",
      " 0.158366      6\n",
      "            ... \n",
      "-0.585882      1\n",
      "-0.238614      1\n",
      "-0.930629      1\n",
      "-0.334732      1\n",
      "-0.825745      1\n",
      "Name: count, Length: 1463, dtype: int64 \n",
      "\n",
      "\n",
      "Histogram for: arousal_predicted\n",
      "arousal_predicted\n",
      "-0.158390    344\n",
      "-0.076169     75\n",
      "-0.158477      6\n",
      "-0.266480      6\n",
      "-0.125051      6\n",
      "            ... \n",
      "-0.367905      1\n",
      " 0.018236      1\n",
      " 0.327953      1\n",
      "-0.091091      1\n",
      "-0.297470      1\n",
      "Name: count, Length: 1463, dtype: int64 \n",
      "\n",
      "\n",
      "Histogram for: bpm\n",
      "bpm\n",
      "130.0    17\n",
      "140.0    16\n",
      "160.2    14\n",
      "119.0    13\n",
      "119.8    12\n",
      "         ..\n",
      "150.3     1\n",
      "97.1      1\n",
      "168.1     1\n",
      "106.0     1\n",
      "208.8     1\n",
      "Name: count, Length: 248, dtype: int64 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3618/3752516917.py:6: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  print(pd.value_counts(df[property]), '\\n\\n')\n",
      "/tmp/ipykernel_3618/3752516917.py:6: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  print(pd.value_counts(df[property]), '\\n\\n')\n",
      "/tmp/ipykernel_3618/3752516917.py:6: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  print(pd.value_counts(df[property]), '\\n\\n')\n",
      "/tmp/ipykernel_3618/3752516917.py:6: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  print(pd.value_counts(df[property]), '\\n\\n')\n",
      "/tmp/ipykernel_3618/3752516917.py:6: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  print(pd.value_counts(df[property]), '\\n\\n')\n"
     ]
    }
   ],
   "source": [
    "### Quick overview over some columns\n",
    "df = load_songs('wasabi_songs.csv', nrows=2000)\n",
    "for property in ['artist', 'title', 'publicationDateAlbum', 'album_genre',\n",
    "                 'valence', 'arousal', 'valence_predicted', 'arousal_predicted', 'bpm']:\n",
    "    print('Histogram for:', property)\n",
    "    print(pd.value_counts(df[property]), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Step: Make new df with all relevant features we want to incorporate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       Unnamed: 0                                 _id abstract  \\\n",
      "0              0  ObjectId(5714dec325ac0d8aee3804e7)      NaN   \n",
      "1              1  ObjectId(5714dec325ac0d8aee3804e8)      NaN   \n",
      "2              2  ObjectId(5714dec325ac0d8aee3804e9)      NaN   \n",
      "3              3  ObjectId(5714dec325ac0d8aee3804ea)      NaN   \n",
      "4              4  ObjectId(5714dec325ac0d8aee3804eb)      NaN   \n",
      "...          ...                                 ...      ...   \n",
      "1995        1995  ObjectId(5714dec325ac0d8aee380cb2)      NaN   \n",
      "1996        1996  ObjectId(5714dec325ac0d8aee380cb3)      NaN   \n",
      "1997        1997  ObjectId(5714dec325ac0d8aee380cb4)      NaN   \n",
      "1998        1998  ObjectId(5714dec325ac0d8aee380cb5)      NaN   \n",
      "1999        1999  ObjectId(5714dec325ac0d8aee380cb6)      NaN   \n",
      "\n",
      "                 albumTitle       album_genre animux_content animux_contents  \\\n",
      "0     How Ace Are Buildings  Alternative Rock            NaN             NaN   \n",
      "1     How Ace Are Buildings  Alternative Rock            NaN             NaN   \n",
      "2     How Ace Are Buildings  Alternative Rock            NaN             NaN   \n",
      "3     How Ace Are Buildings  Alternative Rock            NaN             NaN   \n",
      "4     How Ace Are Buildings  Alternative Rock            NaN             NaN   \n",
      "...                     ...               ...            ...             ...   \n",
      "1995                 Realis               NaN            NaN             NaN   \n",
      "1996                 Realis               NaN            NaN             NaN   \n",
      "1997                 Realis               NaN            NaN             NaN   \n",
      "1998                 Realis               NaN            NaN             NaN   \n",
      "1999                 Realis               NaN            NaN             NaN   \n",
      "\n",
      "     animux_path animux_paths  arousal  ...  \\\n",
      "0            NaN           []      NaN  ...   \n",
      "1            NaN           []      NaN  ...   \n",
      "2            NaN           []      NaN  ...   \n",
      "3            NaN           []      NaN  ...   \n",
      "4            NaN           []      NaN  ...   \n",
      "...          ...          ...      ...  ...   \n",
      "1995         NaN           []      NaN  ...   \n",
      "1996         NaN           []      NaN  ...   \n",
      "1997         NaN           []      NaN  ...   \n",
      "1998         NaN           []      NaN  ...   \n",
      "1999         NaN           []      NaN  ...   \n",
      "\n",
      "                                         urlMusicBrainz urlPandora  \\\n",
      "0     http://musicbrainz.org/recording/3db608e4-eb72...        NaN   \n",
      "1     http://musicbrainz.org/recording/84feea0c-187f...        NaN   \n",
      "2     http://musicbrainz.org/recording/f9303efd-b512...        NaN   \n",
      "3     http://musicbrainz.org/recording/52ef8537-c61d...        NaN   \n",
      "4     http://musicbrainz.org/recording/53ae7abb-e5c8...        NaN   \n",
      "...                                                 ...        ...   \n",
      "1995                                                NaN        NaN   \n",
      "1996                                                NaN        NaN   \n",
      "1997                                                NaN        NaN   \n",
      "1998                                                NaN        NaN   \n",
      "1999                                                NaN        NaN   \n",
      "\n",
      "                                                urlSong urlSpotify  \\\n",
      "0                  http://lyrics.wikia.com/A:Turn_It_Up        NaN   \n",
      "1                     http://lyrics.wikia.com/A:Foghorn        NaN   \n",
      "2               http://lyrics.wikia.com/A:Cheeky_Monkey        NaN   \n",
      "3                       http://lyrics.wikia.com/A:No._1        NaN   \n",
      "4                    http://lyrics.wikia.com/A:Bad_Idea        NaN   \n",
      "...                                                 ...        ...   \n",
      "1995  http://lyrics.wikia.com/A_Hope_For_Home:Wither...        NaN   \n",
      "1996  http://lyrics.wikia.com/A_Hope_For_Home:The_Ma...        NaN   \n",
      "1997   http://lyrics.wikia.com/A_Hope_For_Home:No_Light        NaN   \n",
      "1998  http://lyrics.wikia.com/A_Hope_For_Home:Post_T...        NaN   \n",
      "1999  http://lyrics.wikia.com/A_Hope_For_Home:First_...        NaN   \n",
      "\n",
      "     urlWikipedia urlYouTube  urlYouTubeExist valence valence_predicted writer  \n",
      "0             NaN        NaN              NaN     NaN          0.657853    NaN  \n",
      "1             NaN        NaN              NaN     NaN         -0.810233    NaN  \n",
      "2             NaN        NaN              NaN     NaN          0.223842    NaN  \n",
      "3             NaN        NaN              NaN     NaN         -0.016932    NaN  \n",
      "4             NaN        NaN              NaN     NaN          0.339134    NaN  \n",
      "...           ...        ...              ...     ...               ...    ...  \n",
      "1995          NaN        NaN              NaN     NaN         -0.347331    NaN  \n",
      "1996          NaN        NaN              NaN     NaN         -0.347331    NaN  \n",
      "1997          NaN        NaN              NaN     NaN         -0.347331    NaN  \n",
      "1998          NaN        NaN              NaN     NaN         -0.347331    NaN  \n",
      "1999          NaN        NaN              NaN     NaN         -0.347331    NaN  \n",
      "\n",
      "[2000 rows x 78 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(df.head)\n",
    "features = df[[\"title\",\"artist\", \"album_genre\", \"valence\", \"publicationDateAlbum\"]]\n",
    "\n",
    "X = features\n",
    "y = df[\"bpm\"] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .75)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/joocey/Documents/wasabiwip/wasabi_test.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joocey/Documents/wasabiwip/wasabi_test.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model \u001b[39m=\u001b[39m KerasClassifier(model\u001b[39m=\u001b[39mcreate_model, epochs\u001b[39m=\u001b[39m\u001b[39m150\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joocey/Documents/wasabiwip/wasabi_test.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m kfold \u001b[39m=\u001b[39m StratifiedKFold(n_splits\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39mseed)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/joocey/Documents/wasabiwip/wasabi_test.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m results \u001b[39m=\u001b[39m cross_val_score(model, X_train, y_train, cv\u001b[39m=\u001b[39;49mkfold)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    560\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 562\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    563\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    564\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    565\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    566\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    567\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    568\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    569\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    570\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    571\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    572\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    573\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    574\u001b[0m )\n\u001b[1;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:301\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     scorers \u001b[39m=\u001b[39m _check_multimetric_scoring(estimator, scoring)\n\u001b[0;32m--> 301\u001b[0m indices \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m return_indices:\n\u001b[1;32m    303\u001b[0m     \u001b[39m# materialize the indices since we need to store them in the returned dict\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(indices)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:808\u001b[0m, in \u001b[0;36mStratifiedKFold.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit\u001b[39m(\u001b[39mself\u001b[39m, X, y, groups\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    775\u001b[0m     \u001b[39m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \n\u001b[1;32m    777\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[39m    to an integer.\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 808\u001b[0m     y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    809\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msplit(X, y, groups)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         _assert_all_finite(\n\u001b[1;32m    958\u001b[0m             array,\n\u001b[1;32m    959\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    960\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    961\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    964\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    123\u001b[0m     X,\n\u001b[1;32m    124\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[1;32m    125\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[1;32m    126\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[1;32m    127\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    128\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    129\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    155\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    " # create model\n",
    " model = Sequential()\n",
    " model.add(Dense(12, input_dim=8, activation='relu'))\n",
    " model.add(Dense(8, activation='relu'))\n",
    " model.add(Dense(1, activation='sigmoid'))\n",
    " # Compile model\n",
    " model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " return model\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = KerasClassifier(model=create_model, epochs=150, batch_size=10, verbose=0)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(model, X_train, y_train, cv=kfold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
